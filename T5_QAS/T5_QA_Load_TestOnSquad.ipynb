{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01f92193",
   "metadata": {},
   "source": [
    "# Load T5-base QA (t5-base-finetuned-question-answering) Model and predict on validation data fro SQuAD Question-answering Dataset\n",
    "\n",
    "\n",
    "### Code Reference: \n",
    "\n",
    "https://huggingface.co/MaRiOrOsSi/t5-base-finetuned-question-answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993c8e54-45e1-4cd2-b2bc-afda48339fbe",
   "metadata": {},
   "source": [
    "### 1. Import Packages relevant ot modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42f40dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  transformers  import  AutoTokenizer, AutoModelWithLMHead, pipeline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f82202-c34a-47e3-9730-c9a5e28bf893",
   "metadata": {},
   "source": [
    "### 2. Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea343f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"MaRiOrOsSi/t5-base-finetuned-question-answering\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c83f26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ccb1e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\basharm\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1132: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelWithLMHead.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009e6edd-c3e6-4a9f-ba3b-f7e8b7d86bcf",
   "metadata": {},
   "source": [
    "### 3. Test pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "813da14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. Bashar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\basharm\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "question = \"Who graduated from QUT?\"\n",
    "context = \"The name of the scientist is Dr. Bashar. He graduated from QUT.\"\n",
    "input = f\"question: {question} context: {context}\"\n",
    "encoded_input = tokenizer([input],\n",
    "                             return_tensors='pt',\n",
    "                             max_length=512,\n",
    "                             truncation=True)\n",
    "output = model.generate(input_ids = encoded_input.input_ids,\n",
    "                            attention_mask = encoded_input.attention_mask)\n",
    "output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cdbf45-69d0-4b92-8372-6cb6ed6d2cfe",
   "metadata": {},
   "source": [
    "### 4. Import packages relevant to data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78946838-54ec-4f39-98e1-5579525660bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335a6e6c-34c1-4b39-9bb6-948797f8c8e6",
   "metadata": {},
   "source": [
    "### 5. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fdf127e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/basharm/.cache/huggingface/datasets/parquet/plain_text-d0ce9b3222e19e32/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaaedb46bb5c43b9b1cc3473d491918d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "squad = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d8c81de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 10570\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad['validation']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1825a77-44ff-4960-9d19-f2dbdf21d866",
   "metadata": {},
   "source": [
    "### 7. Define predict funstion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9ff9ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_answer(question, context):\n",
    "    input = f\"question: {question} context: {context}\"\n",
    "    encoded_input = tokenizer([input],\n",
    "                             return_tensors='pt',\n",
    "                             max_length=512,\n",
    "                             truncation=True)\n",
    "    output = model.generate(input_ids = encoded_input.input_ids,\n",
    "                            attention_mask = encoded_input.attention_mask)\n",
    "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c0f923-8045-4e70-bf72-367602a74410",
   "metadata": {},
   "source": [
    "### 8. Make prediction on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee8edc3-ff31-446b-8eac-8107f6465cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "out_rows = []\n",
    "for row in squad['validation']:\n",
    "    question = row['question']\n",
    "    #print(question)\n",
    "    context = row['context']\n",
    "    #print(context)\n",
    "    ans = row['answers']['text'][0]\n",
    "    #print(ans)\n",
    "    pred_ans = predict_answer(question, context)\n",
    "    #print(pred_ans)\n",
    "    out_row = {'answer': ans, 'pred_ans': pred_ans}\n",
    "    out_rows.append(out_row)\n",
    "    count += 1\n",
    "    if count%100==0:\n",
    "        print('Completed Respon Count', count)\n",
    "    \n",
    "df_out_val = pd.DataFrame(out_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0228505c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>pred_ans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Denver Broncos</td>\n",
       "      <td>Denver Broncos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Carolina Panthers</td>\n",
       "      <td>Denver Broncos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Santa Clara, California</td>\n",
       "      <td>Santa Clara, California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Denver Broncos</td>\n",
       "      <td>Denver Broncos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gold</td>\n",
       "      <td>gold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10565</th>\n",
       "      <td>kilogram-force</td>\n",
       "      <td>Kilowatt-force</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10566</th>\n",
       "      <td>kilopond</td>\n",
       "      <td>Kilopond</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10567</th>\n",
       "      <td>slug</td>\n",
       "      <td>metric slug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10568</th>\n",
       "      <td>kip</td>\n",
       "      <td>Kilowatt-force</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10569</th>\n",
       "      <td>sthène</td>\n",
       "      <td>Kilowatt force</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10570 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        answer                 pred_ans\n",
       "0               Denver Broncos           Denver Broncos\n",
       "1            Carolina Panthers           Denver Broncos\n",
       "2      Santa Clara, California  Santa Clara, California\n",
       "3               Denver Broncos           Denver Broncos\n",
       "4                         gold                     gold\n",
       "...                        ...                      ...\n",
       "10565           kilogram-force           Kilowatt-force\n",
       "10566                 kilopond                 Kilopond\n",
       "10567                     slug              metric slug\n",
       "10568                      kip           Kilowatt-force\n",
       "10569                   sthène           Kilowatt force\n",
       "\n",
       "[10570 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_out_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5bc1ed-5418-4b20-82e7-bb286eeab9e3",
   "metadata": {},
   "source": [
    "### 9. Stro predicted asnwers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b406695",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out_val.to_csv('T5_squad_valid.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1d7763-cd78-425a-a024-54bc6f169c6d",
   "metadata": {},
   "source": [
    "### 10. Devine evaluation functions and evaluate the prediction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e942c983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match(pred_tokens, true_tokens):\n",
    "    '''\n",
    "    A straightforward way to check the equality of the two lists in Python \n",
    "    is by using the equality == operator. \n",
    "    When the equality == is used on the list type in Python, \n",
    "    it returns True if the lists are equal and False if they are not.\n",
    "    '''\n",
    "    return int(pred_tokens==true_tokens)\n",
    "\n",
    "def half_exact_match(pred_tokens, true_tokens):\n",
    "    '''\n",
    "    A straightforward way to check the equality of the two lists in Python \n",
    "    is by using the equality == operator. \n",
    "    When the equality == is used on the list type in Python, \n",
    "    it returns True if the lists are equal and False if they are not.\n",
    "    '''\n",
    "    if len(pred_tokens)<=1 or len(true_tokens)<=1:\n",
    "        return int(pred_tokens==true_tokens) \n",
    "    else:\n",
    "        return int(pred_tokens[0]==true_tokens[0] or pred_tokens[-1]==true_tokens[-1])\n",
    "    \n",
    "def any_token_match(pred_tokens, true_tokens):\n",
    "    common_tokens = set(pred_tokens) & set(true_tokens)\n",
    "    return int(len(common_tokens)>0)\n",
    "    \n",
    "\n",
    "def get_prec_rec_f1(pred_tokens, true_tokens):\n",
    "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
    "    if len(pred_tokens) == 0 or len(true_tokens) == 0:\n",
    "        prec = rec = f1 = 1\n",
    "        return prec, rec, f1\n",
    "    \n",
    "    common_tokens = set(pred_tokens) & set(true_tokens)\n",
    "    # if there are no common tokens then f1 = 0\n",
    "    if len(common_tokens) == 0:\n",
    "        prec = rec = f1 = 0\n",
    "        return prec, rec, f1\n",
    "    \n",
    "    prec = len(common_tokens) / len(pred_tokens)\n",
    "    rec = len(common_tokens) / len(true_tokens)\n",
    "    f1 = 2 * (prec * rec) / (prec + rec)\n",
    "    \n",
    "    return prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bf6636c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========2025-02-21 14:52:02.533026========\n",
      "=========2025-02-21 14:52:02.533026========\n",
      "exact_match: 0.38420056764427624\n",
      "half_exact_match: 0.584484389782403\n",
      "any_match: 0.7371807000946073\n",
      "recall: 0.6144846712364499\n",
      "precision: 0.6243597824517078\n",
      "f1: 0.6001928249863442\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "match = 0\n",
    "half_match = 0\n",
    "any_match = 0\n",
    "prec = 0\n",
    "rec = 0\n",
    "f1 = 0\n",
    "count = 0\n",
    "for idx in df_out_val.index:\n",
    "\n",
    "    pred_tokens = df_out_val.loc[idx, 'pred_ans'].split()\n",
    "    true_tokens = df_out_val.loc[idx, 'answer'].split()\n",
    "    \n",
    "    match +=  exact_match(pred_tokens, true_tokens)\n",
    "    half_match += half_exact_match(pred_tokens, true_tokens)\n",
    "    any_match += any_token_match(pred_tokens, true_tokens)\n",
    "    \n",
    "    score =  get_prec_rec_f1(pred_tokens, true_tokens)\n",
    "    prec += score[0]\n",
    "    rec += score[1]\n",
    "    f1 += score[2]\n",
    "    count += 1\n",
    "\n",
    "    \n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "string = ''\n",
    "string += '========={}========\\n'.format(now)\n",
    "string += '========={}========\\n'.format(now)\n",
    "string += 'exact_match: '+str(match/count)+'\\n'\n",
    "string += 'half_exact_match: '+str(half_match/count)+'\\n'\n",
    "string += 'any_match: '+str(any_match/count)+'\\n'\n",
    "string += 'recall: '+str(rec/count)+'\\n'\n",
    "string += 'precision: '+str(prec/count)+'\\n'\n",
    "string += 'f1: '+str(f1/count)+'\\n\\n'\n",
    "print(string)\n",
    "\n",
    "with open('Report_T5_Squad.txt', 'a+') as FO:\n",
    "    FO.write(string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
