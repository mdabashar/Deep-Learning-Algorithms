{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "695e8202",
   "metadata": {},
   "source": [
    "### RoBERTa (PyTorch) for Disaster Tweets Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab0d116",
   "metadata": {},
   "source": [
    "##### Data Source: https://www.kaggle.com/competitions/nlp-getting-started/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f71d4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e38852",
   "metadata": {},
   "source": [
    "### Read data to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b1acb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = 'D:\\\\ResearchDataGtx1060\\\\DisasterTweets\\\\'\n",
    "\n",
    "train_df = pd.read_csv(BASE+\"train.csv\")\n",
    "test_df = pd.read_csv(BASE+\"test.csv\")\n",
    "\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "007fea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1401e259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>7128</td>\n",
       "      <td>Courageous and honest analysis of need to use ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3263</th>\n",
       "      <td>4688</td>\n",
       "      <td>@ZachZaidman @670TheScore wld b a shame if tha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4907</th>\n",
       "      <td>6984</td>\n",
       "      <td>Tell @BarackObama to rescind medals of 'honor'...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2855</th>\n",
       "      <td>4103</td>\n",
       "      <td>Worried about how the CA drought might affect ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4716</th>\n",
       "      <td>6706</td>\n",
       "      <td>@YoungHeroesID Lava Blast &amp;amp; Power Red #Pan...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5226</th>\n",
       "      <td>7470</td>\n",
       "      <td>@Eganator2000 There aren't many Obliteration s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>7691</td>\n",
       "      <td>just had a panic attack bc I don't have enough...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>1242</td>\n",
       "      <td>Omron HEM-712C Automatic Blood Pressure Monito...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7603</th>\n",
       "      <td>10862</td>\n",
       "      <td>Officials say a quarantine is in place at an A...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7270</th>\n",
       "      <td>10409</td>\n",
       "      <td>I moved to England five years ago today. What ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6090 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                               text  target\n",
       "4996   7128  Courageous and honest analysis of need to use ...       1\n",
       "3263   4688  @ZachZaidman @670TheScore wld b a shame if tha...       0\n",
       "4907   6984  Tell @BarackObama to rescind medals of 'honor'...       1\n",
       "2855   4103  Worried about how the CA drought might affect ...       1\n",
       "4716   6706  @YoungHeroesID Lava Blast &amp; Power Red #Pan...       0\n",
       "...     ...                                                ...     ...\n",
       "5226   7470  @Eganator2000 There aren't many Obliteration s...       0\n",
       "5390   7691  just had a panic attack bc I don't have enough...       0\n",
       "860    1242  Omron HEM-712C Automatic Blood Pressure Monito...       0\n",
       "7603  10862  Officials say a quarantine is in place at an A...       1\n",
       "7270  10409  I moved to England five years ago today. What ...       1\n",
       "\n",
       "[6090 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df.drop([\"keyword\", \"location\"], axis=1)\n",
    "val_df = val_df.drop([\"keyword\", \"location\"], axis=1)\n",
    "test_df = test_df.drop([\"keyword\", \"location\"], axis=1)\n",
    "\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968cb793",
   "metadata": {},
   "source": [
    "### Data Loading and Preprocessing\n",
    "\n",
    "We remove mentions, links, hashtags, punctuations, and other stuff we deem not necessary for our model to come up with correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc7e72d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        texts = dataframe.text.values.tolist()\n",
    "\n",
    "        texts = [self._preprocess(text) for text in texts]\n",
    "\n",
    "        self._print_random_samples(texts)\n",
    "\n",
    "        self.texts = [tokenizer(text, padding='max_length',\n",
    "                                max_length=150,\n",
    "                                truncation=True,\n",
    "                                return_tensors=\"pt\")\n",
    "                      for text in texts]\n",
    "\n",
    "        if 'target' in dataframe:\n",
    "            classes = dataframe.target.values.tolist()\n",
    "            self.labels = classes\n",
    "\n",
    "    def _print_random_samples(self, texts):\n",
    "        np.random.seed(42)\n",
    "        random_entries = np.random.randint(0, len(texts), 5)\n",
    "\n",
    "        for i in random_entries:\n",
    "            print(f\"Entry {i}: {texts[i]}\")\n",
    "\n",
    "        print()\n",
    "\n",
    "    def _preprocess(self, text):\n",
    "        text = self._remove_amp(text)\n",
    "        text = self._remove_links(text)\n",
    "        text = self._remove_hashes(text)\n",
    "        text = self._remove_retweets(text)\n",
    "        text = self._remove_mentions(text)\n",
    "        text = self._remove_multiple_spaces(text)\n",
    "\n",
    "        #text = self._lowercase(text)\n",
    "        text = self._remove_punctuation(text)\n",
    "        #text = self._remove_numbers(text)\n",
    "\n",
    "        text_tokens = self._tokenize(text)\n",
    "        text_tokens = self._stopword_filtering(text_tokens)\n",
    "        #text_tokens = self._stemming(text_tokens)\n",
    "        text = self._stitch_text_tokens_together(text_tokens)\n",
    "\n",
    "        return text.strip()\n",
    "\n",
    "\n",
    "    def _remove_amp(self, text):\n",
    "        return text.replace(\"&amp;\", \" \")\n",
    "\n",
    "    def _remove_mentions(self, text):\n",
    "        return re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "    \n",
    "    def _remove_multiple_spaces(self, text):\n",
    "        return re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    def _remove_retweets(self, text):\n",
    "        return re.sub(r'^RT[\\s]+', ' ', text)\n",
    "\n",
    "    def _remove_links(self, text):\n",
    "        return re.sub(r'https?:\\/\\/[^\\s\\n\\r]+', ' ', text)\n",
    "\n",
    "    def _remove_hashes(self, text):\n",
    "        return re.sub(r'#', ' ', text)\n",
    "\n",
    "    def _stitch_text_tokens_together(self, text_tokens):\n",
    "        return \" \".join(text_tokens)\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        return nltk.word_tokenize(text, language=\"english\")\n",
    "\n",
    "    def _stopword_filtering(self, text_tokens):\n",
    "        stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "        return [token for token in text_tokens if token not in stop_words]\n",
    "\n",
    "    def _stemming(self, text_tokens):\n",
    "        porter = nltk.stem.porter.PorterStemmer()\n",
    "        return [porter.stem(token) for token in text_tokens]\n",
    "\n",
    "    def _remove_numbers(self, text):\n",
    "        return re.sub(r'\\d+', ' ', text)\n",
    "\n",
    "    def _lowercase(self, text):\n",
    "        return text.lower()\n",
    "\n",
    "    def _remove_punctuation(self, text):\n",
    "        return ''.join(character for character in text if character not in string.punctuation)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "\n",
    "        label = -1\n",
    "        if hasattr(self, 'labels'):\n",
    "            label = self.labels[idx]\n",
    "\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2afe26a",
   "metadata": {},
   "source": [
    "### Building the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d74cca60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetClassifier(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(TweetClassifier, self).__init__()\n",
    "\n",
    "        self.bert = base_model\n",
    "        self.fc1 = nn.Linear(768, 32)\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        bert_out = self.bert(input_ids=input_ids,\n",
    "                             attention_mask=attention_mask)[0][:, 0]\n",
    "        x = self.fc1(bert_out)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d80bee5",
   "metadata": {},
   "source": [
    "### Setting up the training loop\n",
    "\n",
    "For each epoch we calculate the loss as well as the accuracy on the train and the validation set. We apply early stopping and store the best model, according to the validation loss, for later usage.\n",
    "\n",
    "We use BinaryCrossEntropyLoss as we are dealing with a binary classification task, and our model is built in such a way that via the Sigmoid function at the end it should output a nice probability of 0 (=no disaster) and 1 (=disaster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c00222f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, val_dataloader, learning_rate, epochs):\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_threshold_count = 0\n",
    "    \n",
    "    \n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        for train_input, train_label in tqdm(train_dataloader):\n",
    "            attention_mask = train_input['attention_mask'].to(device)\n",
    "            input_ids = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            train_label = train_label.to(device)\n",
    "\n",
    "            output = model(input_ids, attention_mask)\n",
    "\n",
    "            loss = criterion(output, train_label.float().unsqueeze(1))\n",
    "\n",
    "            total_loss_train += loss.item()\n",
    "\n",
    "            acc = ((output >= 0.5).int() == train_label.unsqueeze(1)).sum().item()\n",
    "            total_acc_train += acc\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            for val_input, val_label in tqdm(val_dataloader):\n",
    "                attention_mask = val_input['attention_mask'].to(device)\n",
    "                input_ids = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                val_label = val_label.to(device)\n",
    "\n",
    "                output = model(input_ids, attention_mask)\n",
    "\n",
    "                loss = criterion(output, val_label.float().unsqueeze(1))\n",
    "\n",
    "                total_loss_val += loss.item()\n",
    "\n",
    "                acc = ((output >= 0.5).int() == val_label.unsqueeze(1)).sum().item()\n",
    "                total_acc_val += acc\n",
    "            \n",
    "            print(f'Epochs: {epoch + 1} '\n",
    "                  f'| Train Loss: {total_loss_train / len(train_dataloader): .3f} '\n",
    "                  f'| Train Accuracy: {total_acc_train / (len(train_dataloader.dataset)): .3f} '\n",
    "                  f'| Val Loss: {total_loss_val / len(val_dataloader): .3f} '\n",
    "                  f'| Val Accuracy: {total_acc_val / len(val_dataloader.dataset): .3f}')\n",
    "            \n",
    "            if best_val_loss > total_loss_val:\n",
    "                best_val_loss = total_loss_val\n",
    "                torch.save(model, f\"best_model.pt\")\n",
    "                print(\"Saved model\")\n",
    "                early_stopping_threshold_count = 0\n",
    "            else:\n",
    "                early_stopping_threshold_count += 1\n",
    "                \n",
    "            if early_stopping_threshold_count >= 1:\n",
    "                print(\"Early stopping\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06bb94e",
   "metadata": {},
   "source": [
    "### Training our classifier\n",
    "\n",
    "As our final step for the text classification we put it all together, we initialize our train and validation datasets, instantiate our chosen RoBERTa model and start a training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bab4923f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry 860: If NWS wth rotating storm w report HUGE MASSIVE VIOLENT tornado would\n",
      "Entry 5390: Demolished My Personal Best\n",
      "Entry 5226: sing tsunami Beginners computer tutorial Everyone Wants To Learn To Build A Pc Re\n",
      "Entry 5191: Survival Kit Whistle Fire Starter Wire Saw Cree Torch Emergency Blanket S knife Full reÛ\n",
      "Entry 3772: Buddha man time massive urbanisation social upheaval also challenged Brahmans dominance Genius Ancient World\n",
      "\n",
      "Entry 1126: They didnt succeed two times either Bomb didnt detonate Shots missed\n",
      "Entry 1459: Nobody remembers came second Charles Schulz\n",
      "Entry 860: Near sand half sunk shattered visage lies\n",
      "Entry 1294: understanding umntu wakho If trust partner OK u know wont fear anything\n",
      "Entry 1130: My back sunburned\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 762/762 [04:36<00:00,  2.76it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 191/191 [00:18<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.457 | Train Accuracy:  0.796 | Val Loss:  0.384 | Val Accuracy:  0.842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/762 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 762/762 [04:39<00:00,  2.72it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 191/191 [00:18<00:00, 10.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.361 | Train Accuracy:  0.853 | Val Loss:  0.391 | Val Accuracy:  0.836\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "    \n",
    "    \n",
    "BERT_MODEL = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL)\n",
    "base_model = AutoModel.from_pretrained(BERT_MODEL)\n",
    "\n",
    "train_dataloader = DataLoader(TweetDataset(train_df, tokenizer), batch_size=8, shuffle=True, num_workers=0)\n",
    "val_dataloader = DataLoader(TweetDataset(val_df, tokenizer), batch_size=8, num_workers=0)\n",
    "\n",
    "model = TweetClassifier(base_model)\n",
    "\n",
    "\n",
    "learning_rate = 1e-5\n",
    "epochs = 5\n",
    "train(model, train_dataloader, val_dataloader, learning_rate, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5a307f",
   "metadata": {},
   "source": [
    "### Predicting for the test data\n",
    "\n",
    "Analogously to the training loop, we define ourselves a method to conveniently extract the predictions done by our model. As our model is returning a probability between 0 and 1 we use a 50% threshold for our target classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7882744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_predictions(model, loader):\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    \n",
    "    \n",
    "    results_predictions = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for data_input, _ in tqdm(loader):\n",
    "            attention_mask = data_input['attention_mask'].to(device)\n",
    "            input_ids = data_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "\n",
    "            output = model(input_ids, attention_mask)\n",
    "            \n",
    "            output = (output > 0.5).int()\n",
    "            results_predictions.append(output)\n",
    "    \n",
    "    return torch.cat(results_predictions).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e442143c",
   "metadata": {},
   "source": [
    "Next we load up the previously saved model and set up the test data loader.\n",
    "\n",
    "To store our predictions we use the given \"sample_submission.csv\", which already contains the test IDs and the sample target, which we are going to overwrite with the predictions of our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b7405a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry 3174: Rocky Fire cali SCFD wildfire LakeCounty\n",
      "Entry 860: First time everything Coney Island Cyclone\n",
      "Entry 1294: If told drowning I would lend hand\n",
      "Entry 1130: HitchBot travels Europe greeted open arms Gets destroyed two weeks america Theres lesson learned\n",
      "Entry 1095: Free Kindle Book Aug 37 Thriller Desolation Run\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 408/408 [00:38<00:00, 10.73it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  target\n",
       "0    0       1\n",
       "1    2       1\n",
       "2    3       1\n",
       "3    9       1\n",
       "4   11       1\n",
       "5   12       0\n",
       "6   21       0\n",
       "7   22       0\n",
       "8   27       0\n",
       "9   29       0\n",
       "10  30       0\n",
       "11  35       0\n",
       "12  42       0\n",
       "13  43       0\n",
       "14  45       0\n",
       "15  46       1\n",
       "16  47       0\n",
       "17  51       0\n",
       "18  58       0\n",
       "19  60       0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = torch.load(\"best_model.pt\")\n",
    "\n",
    "test_dataloader = DataLoader(TweetDataset(test_df, tokenizer), \n",
    "batch_size=8, shuffle=False, num_workers=0)\n",
    "\n",
    "sample_submission = pd.read_csv(BASE+\"sample_submission.csv\")\n",
    "\n",
    "sample_submission[\"target\"] = get_text_predictions(model, test_dataloader)\n",
    "\n",
    "display(sample_submission.head(20))\n",
    "\n",
    "sample_submission.to_csv(\"submission_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e049644",
   "metadata": {},
   "source": [
    "Reference: \n",
    "\n",
    "https://www.auroria.io/nlp-disaster-tweet-text-classification-roberta-pytorch/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
